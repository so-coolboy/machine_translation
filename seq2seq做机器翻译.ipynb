{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq做机器翻译.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYvMsya10ZJW",
        "outputId": "a0b66590-832c-45bb-e872-deb2b8542460"
      },
      "source": [
        "%tensorflow_version 1.x\r\n",
        "import tensorflow as tf\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyhX9ak-uhfD",
        "outputId": "10015ead-1e81-460b-8d72-b4df7aba8725"
      },
      "source": [
        "# 首先克隆一下官方库\r\n",
        "!git clone https://github.com/tensorflow/nmt/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nmt'...\n",
            "remote: Enumerating objects: 1283, done.\u001b[K\n",
            "remote: Total 1283 (delta 0), reused 0 (delta 0), pack-reused 1283\u001b[K\n",
            "Receiving objects: 100% (1283/1283), 1.24 MiB | 19.84 MiB/s, done.\n",
            "Resolving deltas: 100% (918/918), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-fALFV1vaTT",
        "outputId": "73af532d-57d1-4aee-a69e-de68d1a262df"
      },
      "source": [
        "# 查看一下数据加载脚本\r\n",
        "!cat /content/nmt/nmt/scripts/download_iwslt15.sh"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#!/bin/sh\n",
            "# Download small-scale IWSLT15 Vietnames to English translation data for NMT\n",
            "# model training.\n",
            "#\n",
            "# Usage:\n",
            "#   ./download_iwslt15.sh path-to-output-dir\n",
            "#\n",
            "# If output directory is not specified, \"./iwslt15\" will be used as the default\n",
            "# output directory.\n",
            "OUT_DIR=\"${1:-iwslt15}\"\n",
            "SITE_PREFIX=\"https://nlp.stanford.edu/projects/nmt/data\"\n",
            "\n",
            "mkdir -v -p $OUT_DIR\n",
            "\n",
            "# Download iwslt15 small dataset from standford website.\n",
            "echo \"Download training dataset train.en and train.vi.\"\n",
            "curl -o \"$OUT_DIR/train.en\" \"$SITE_PREFIX/iwslt15.en-vi/train.en\"\n",
            "curl -o \"$OUT_DIR/train.vi\" \"$SITE_PREFIX/iwslt15.en-vi/train.vi\"\n",
            "\n",
            "echo \"Download dev dataset tst2012.en and tst2012.vi.\"\n",
            "curl -o \"$OUT_DIR/tst2012.en\" \"$SITE_PREFIX/iwslt15.en-vi/tst2012.en\"\n",
            "curl -o \"$OUT_DIR/tst2012.vi\" \"$SITE_PREFIX/iwslt15.en-vi/tst2012.vi\"\n",
            "\n",
            "echo \"Download test dataset tst2013.en and tst2013.vi.\"\n",
            "curl -o \"$OUT_DIR/tst2013.en\" \"$SITE_PREFIX/iwslt15.en-vi/tst2013.en\"\n",
            "curl -o \"$OUT_DIR/tst2013.vi\" \"$SITE_PREFIX/iwslt15.en-vi/tst2013.vi\"\n",
            "\n",
            "echo \"Download vocab file vocab.en and vocab.vi.\"\n",
            "curl -o \"$OUT_DIR/vocab.en\" \"$SITE_PREFIX/iwslt15.en-vi/vocab.en\"\n",
            "curl -o \"$OUT_DIR/vocab.vi\" \"$SITE_PREFIX/iwslt15.en-vi/vocab.vi\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d245iC_LvyEY",
        "outputId": "6478d1e2-2d5a-44df-e0f8-4a13b7be20a8"
      },
      "source": [
        "# 运行脚本，下载数据\r\n",
        "!/content/nmt/nmt/scripts/download_iwslt15.sh ./data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: created directory './data'\n",
            "Download training dataset train.en and train.vi.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 12.9M  100 12.9M    0     0  15.5M      0 --:--:-- --:--:-- --:--:-- 15.5M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 17.2M  100 17.2M    0     0  18.8M      0 --:--:-- --:--:-- --:--:-- 18.8M\n",
            "Download dev dataset tst2012.en and tst2012.vi.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  136k  100  136k    0     0   511k      0 --:--:-- --:--:-- --:--:--  511k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  183k  100  183k    0     0   643k      0 --:--:-- --:--:-- --:--:--  641k\n",
            "Download test dataset tst2013.en and tst2013.vi.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  129k  100  129k    0     0   510k      0 --:--:-- --:--:-- --:--:--  510k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  179k  100  179k    0     0   641k      0 --:--:-- --:--:-- --:--:--  641k\n",
            "Download vocab file vocab.en and vocab.vi.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  136k  100  136k    0     0   524k      0 --:--:-- --:--:-- --:--:--  524k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 46767  100 46767    0     0   245k      0 --:--:-- --:--:-- --:--:--  244k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX9yciPVwPAh",
        "outputId": "58966c09-4ea2-42eb-d40d-7a9254cac03e"
      },
      "source": [
        "# 看一下英文训练集的前10行\r\n",
        "!head -10 ./data/train.en"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rachel Pike : The science behind a climate headline\n",
            "In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\n",
            "I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n",
            "Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n",
            "They are both two branches of the same field of atmospheric science .\n",
            "Recently the headlines looked like this when the Intergovernmental Panel on Climate Change , or IPCC , put out their report on the state of understanding of the atmospheric system .\n",
            "That report was written by 620 scientists from 40 countries .\n",
            "They wrote almost a thousand pages on the topic .\n",
            "And all of those pages were reviewed by another 400-plus scientists and reviewers , from 113 countries .\n",
            "It &apos;s a big community . It &apos;s such a big community , in fact , that our annual gathering is the largest scientific meeting in the world .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGh-nE8Pwsb3",
        "outputId": "69279cee-7352-4ada-b78b-dad24cab69e8"
      },
      "source": [
        "# 看一下越南语训练集的前10行\r\n",
        "!head -10 ./data/train.vi"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Khoa học đằng sau một tiêu đề về khí hậu\n",
            "Trong 4 phút , chuyên gia hoá học khí quyển Rachel Pike giới thiệu sơ lược về những nỗ lực khoa học miệt mài đằng sau những tiêu đề táo bạo về biến đổi khí hậu , cùng với đoàn nghiên cứu của mình -- hàng ngàn người đã cống hiến cho dự án này -- một chuyến bay mạo hiểm qua rừng già để tìm kiếm thông tin về một phân tử then chốt .\n",
            "Tôi muốn cho các bạn biết về sự to lớn của những nỗ lực khoa học đã góp phần làm nên các dòng tít bạn thường thấy trên báo .\n",
            "Có những dòng trông như thế này khi bàn về biến đổi khí hậu , và như thế này khi nói về chất lượng không khí hay khói bụi .\n",
            "Cả hai đều là một nhánh của cùng một lĩnh vực trong ngành khoa học khí quyển .\n",
            "Các tiêu đề gần đây trông như thế này khi Ban Điều hành Biến đổi khí hậu Liên chính phủ , gọi tắt là IPCC đưa ra bài nghiên cứu của họ về hệ thống khí quyển .\n",
            "Nghiên cứu được viết bởi 620 nhà khoa học từ 40 quốc gia khác nhau .\n",
            "Họ viết gần 1000 trang về chủ đề này .\n",
            "Và tất cả các trang đều được xem xét bởi 400 khoa học gia và nhà phê bình khác từ 113 quốc gia .\n",
            "Đó là cả một cộng đồng lớn , lớn đến nỗi trên thực tế cuộc tụ hội hằng năm của chúng tôi là hội nghị khoa học &#91; tự nhiên &#93; lớn nhất thế giới .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5ZmFVfcw3JP",
        "outputId": "4359369e-f01c-44c6-bc46-93e9219c5747"
      },
      "source": [
        "# 看一下英文词汇的前10行\r\n",
        "!head -10 ./data/vocab.en"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk>\n",
            "<s>\n",
            "</s>\n",
            "Rachel\n",
            ":\n",
            "The\n",
            "science\n",
            "behind\n",
            "a\n",
            "climate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqB0dKIaxHWv",
        "outputId": "98ade37a-f048-49c4-cb73-84a7806ae640"
      },
      "source": [
        "# 进行训练\r\n",
        "# 不同的attention的方式\r\n",
        "# 源语言是越南语，目标语言是英语，这是后缀\r\n",
        "# 词表\r\n",
        "# 训练集\r\n",
        "# 验证集\r\n",
        "# 测试集\r\n",
        "# 模型输出位置 \r\n",
        "# 训练步数\r\n",
        "!python3 -m nmt.nmt.nmt \\\r\n",
        "    --attention=scaled_luong \\\r\n",
        "    --src=vi --tgt=en \\\r\n",
        "    --vocab_prefix=./data/vocab  \\\r\n",
        "    --train_prefix=./data/train \\\r\n",
        "    --dev_prefix=./data/tst2012  \\\r\n",
        "    --test_prefix=./data/tst2013 \\\r\n",
        "    --out_dir=/tmp/nmt_attention_model \\\r\n",
        "    --num_train_steps=12000 \\\r\n",
        "    --steps_per_stats=100 \\\r\n",
        "    --num_layers=2 \\\r\n",
        "    --num_units=128 \\\r\n",
        "    --dropout=0.2 \\\r\n",
        "    --metrics=bleu"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/nmt/nmt/nmt.py:707: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0129 06:26:09.327220 139824990775168 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "I0129 06:26:10.478821 139824990775168 utils.py:157] NumExpr defaulting to 2 threads.\n",
            "# Job id 0\n",
            "WARNING:tensorflow:From /content/nmt/nmt/nmt.py:629: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0129 06:26:11.447041 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/nmt.py:629: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-01-29 06:26:11.452941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-01-29 06:26:11.515056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:11.515628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-01-29 06:26:11.515917: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-01-29 06:26:11.778109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-01-29 06:26:11.909989: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-01-29 06:26:11.942741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-01-29 06:26:12.196605: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-01-29 06:26:12.248886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-01-29 06:26:12.764434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-01-29 06:26:12.764626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:12.765239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:12.765792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-01-29 06:26:12.834565: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-01-29 06:26:12.834813: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27a6f40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-01-29 06:26:12.834844: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-01-29 06:26:12.966891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:12.967584: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27a7100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-01-29 06:26:12.967613: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-01-29 06:26:12.968329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:12.968846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-01-29 06:26:12.968920: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-01-29 06:26:12.968946: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-01-29 06:26:12.968969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-01-29 06:26:12.968995: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-01-29 06:26:12.969020: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-01-29 06:26:12.969040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-01-29 06:26:12.969061: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-01-29 06:26:12.969135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:12.969719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:12.970194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-01-29 06:26:12.973335: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-01-29 06:26:12.974654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-01-29 06:26:12.974683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-01-29 06:26:12.974693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-01-29 06:26:12.975862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:12.976440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:12.976953: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-01-29 06:26:12.976997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 7702001061069262959), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 13334425438286950854), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 14300559871251609904), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 14912199066, 3193470132320133305)]\n",
            "WARNING:tensorflow:From /content/nmt/nmt/nmt.py:640: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n",
            "W0129 06:26:12.978160 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/nmt.py:640: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n",
            "# Creating output directory /tmp/nmt_attention_model ...\n",
            "WARNING:tensorflow:From /content/nmt/nmt/nmt.py:642: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0129 06:26:12.978523 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/nmt.py:642: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "# Vocab file ./data/vocab.vi exists\n",
            "WARNING:tensorflow:From /content/nmt/nmt/utils/vocab_utils.py:103: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0129 06:26:12.978895 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/utils/vocab_utils.py:103: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "# Vocab file ./data/vocab.en exists\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  saving hparams to /tmp/nmt_attention_model/best_bleu/hparams\n",
            "  attention=scaled_luong\n",
            "  attention_architecture=standard\n",
            "  avg_ckpts=False\n",
            "  batch_size=128\n",
            "  beam_width=0\n",
            "  best_bleu=0\n",
            "  best_bleu_dir=/tmp/nmt_attention_model/best_bleu\n",
            "  check_special_token=True\n",
            "  colocate_gradients_with_ops=True\n",
            "  coverage_penalty_weight=0.0\n",
            "  decay_scheme=\n",
            "  dev_prefix=./data/tst2012\n",
            "  dropout=0.2\n",
            "  embed_prefix=None\n",
            "  encoder_type=uni\n",
            "  eos=</s>\n",
            "  epoch_step=0\n",
            "  forget_bias=1.0\n",
            "  infer_batch_size=32\n",
            "  infer_mode=greedy\n",
            "  init_op=uniform\n",
            "  init_weight=0.1\n",
            "  language_model=False\n",
            "  learning_rate=1.0\n",
            "  length_penalty_weight=0.0\n",
            "  log_device_placement=False\n",
            "  max_gradient_norm=5.0\n",
            "  max_train=0\n",
            "  metrics=['bleu']\n",
            "  num_buckets=5\n",
            "  num_dec_emb_partitions=0\n",
            "  num_decoder_layers=2\n",
            "  num_decoder_residual_layers=0\n",
            "  num_embeddings_partitions=0\n",
            "  num_enc_emb_partitions=0\n",
            "  num_encoder_layers=2\n",
            "  num_encoder_residual_layers=0\n",
            "  num_gpus=1\n",
            "  num_inter_threads=0\n",
            "  num_intra_threads=0\n",
            "  num_keep_ckpts=5\n",
            "  num_sampled_softmax=0\n",
            "  num_train_steps=12000\n",
            "  num_translations_per_input=1\n",
            "  num_units=128\n",
            "  optimizer=sgd\n",
            "  out_dir=/tmp/nmt_attention_model\n",
            "  output_attention=True\n",
            "  override_loaded_hparams=False\n",
            "  pass_hidden_state=True\n",
            "  random_seed=None\n",
            "  residual=False\n",
            "  sampling_temperature=0.0\n",
            "  share_vocab=False\n",
            "  sos=<s>\n",
            "  src=vi\n",
            "  src_embed_file=\n",
            "  src_max_len=50\n",
            "  src_max_len_infer=None\n",
            "  src_vocab_file=./data/vocab.vi\n",
            "  src_vocab_size=7709\n",
            "  steps_per_external_eval=None\n",
            "  steps_per_stats=100\n",
            "  subword_option=\n",
            "  test_prefix=./data/tst2013\n",
            "  tgt=en\n",
            "  tgt_embed_file=\n",
            "  tgt_max_len=50\n",
            "  tgt_max_len_infer=None\n",
            "  tgt_vocab_file=./data/vocab.en\n",
            "  tgt_vocab_size=17191\n",
            "  time_major=True\n",
            "  train_prefix=./data/train\n",
            "  unit_type=lstm\n",
            "  use_char_encode=False\n",
            "  vocab_prefix=./data/vocab\n",
            "  warmup_scheme=t2t\n",
            "  warmup_steps=0\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:90: The name tf.container is deprecated. Please use tf.compat.v1.container instead.\n",
            "\n",
            "W0129 06:26:13.013089 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:90: The name tf.container is deprecated. Please use tf.compat.v1.container instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:94: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0129 06:26:13.021271 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:94: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:96: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0129 06:26:13.044943 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:96: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.<lambda> at 0x7f2b19e06510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:13.067209 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.<lambda> at 0x7f2b19e06510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.<lambda> at 0x7f2b19e068c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:13.083311 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.<lambda> at 0x7f2b19e068c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.<lambda> at 0x7f2b19e06c80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0129 06:26:13.092524 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.<lambda> at 0x7f2b19e06c80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.<lambda> at 0x7f2b1b093488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0129 06:26:13.107995 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.<lambda> at 0x7f2b1b093488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.<lambda> at 0x7f2b1b0939d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:13.120194 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.<lambda> at 0x7f2b1b0939d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.<lambda> at 0x7f2b1b093ea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:13.133451 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.<lambda> at 0x7f2b1b093ea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.<lambda> at 0x7f2b19efb2f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:13.146735 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.<lambda> at 0x7f2b19efb2f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:From /content/nmt/nmt/utils/iterator_utils.py:235: group_by_window (from tensorflow.contrib.data.python.ops.grouping) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.group_by_window(...)`.\n",
            "W0129 06:26:13.152197 139824990775168 deprecation.py:323] From /content/nmt/nmt/utils/iterator_utils.py:235: group_by_window (from tensorflow.contrib.data.python.ops.grouping) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.group_by_window(...)`.\n",
            "WARNING:tensorflow:From /content/nmt/nmt/utils/iterator_utils.py:228: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0129 06:26:13.156947 139824990775168 deprecation.py:323] From /content/nmt/nmt/utils/iterator_utils.py:228: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.reduce_func at 0x7f2b19efb378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0129 06:26:13.163566 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.reduce_func at 0x7f2b19efb378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From /content/nmt/nmt/utils/iterator_utils.py:239: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W0129 06:26:13.175930 139824990775168 deprecation.py:323] From /content/nmt/nmt/utils/iterator_utils.py:239: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:162: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "W0129 06:26:13.183901 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model.py:162: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0129 06:26:13.184138 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:285: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0129 06:26:13.184503 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:285: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "# Creating train graph ...\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:375: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n",
            "W0129 06:26:13.195888 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model.py:375: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n",
            "# Build a basic encoder\n",
            "  num_layers = 2, num_residual_layers=0\n",
            "  cell 0  LSTM, forget_bias=1WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:402: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0129 06:26:13.200945 139824990775168 deprecation.py:323] From /content/nmt/nmt/model_helper.py:402: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:508: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "W0129 06:26:13.209425 139824990775168 deprecation.py:323] From /content/nmt/nmt/model_helper.py:508: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:767: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0129 06:26:13.209839 139824990775168 deprecation.py:323] From /content/nmt/nmt/model.py:767: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "W0129 06:26:13.288217 139824990775168 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0129 06:26:13.294362 139824990775168 deprecation.py:506] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0129 06:26:13.335063 139824990775168 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:445: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0129 06:26:13.356111 139824990775168 deprecation.py:323] From /content/nmt/nmt/model.py:445: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:445: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0129 06:26:13.357933 139824990775168 deprecation.py:323] From /content/nmt/nmt/model.py:445: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:190: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "W0129 06:26:13.922643 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model.py:190: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "  learning_rate=1, warmup_steps=0, warmup_scheme=t2t\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:248: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0129 06:26:13.923464 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model.py:248: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "  decay_scheme=, start_decay_step=12000, decay_steps 0, decay_factor 1\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:295: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "W0129 06:26:13.935580 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model.py:295: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:203: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
            "\n",
            "W0129 06:26:13.942019 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model.py:203: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:515: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0129 06:26:15.040988 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:515: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:517: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.\n",
            "\n",
            "W0129 06:26:15.042246 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:517: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:321: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0129 06:26:15.067076 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model.py:321: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "# Trainable variables\n",
            "Format: <name>, <shape>, <(soft) device placement>\n",
            "  embeddings/encoder/embedding_encoder:0, (7709, 128), /device:GPU:0\n",
            "  embeddings/decoder/embedding_decoder:0, (17191, 128), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 17191), /device:GPU:0\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:100: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0129 06:26:15.069730 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model.py:100: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:101: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "W0129 06:26:15.069897 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model.py:101: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.<lambda> at 0x7f2abc15e620> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:15.118109 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.<lambda> at 0x7f2abc15e620> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.<lambda> at 0x7f2abc15e9d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:15.131807 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.<lambda> at 0x7f2abc15e9d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.<lambda> at 0x7f2abc15ed08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:15.143168 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.<lambda> at 0x7f2abc15ed08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.<lambda> at 0x7f2abc187158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:15.156403 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.<lambda> at 0x7f2abc187158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.<lambda> at 0x7f2abc187488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:15.169922 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.<lambda> at 0x7f2abc187488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_iterator.<locals>.reduce_func at 0x7f2abc1878c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0129 06:26:15.186143 139824990775168 ag_logging.py:146] Entity <function get_iterator.<locals>.reduce_func at 0x7f2abc1878c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "# Creating eval graph ...\n",
            "# Build a basic encoder\n",
            "  num_layers = 2, num_residual_layers=0\n",
            "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "# Trainable variables\n",
            "Format: <name>, <shape>, <(soft) device placement>\n",
            "  embeddings/encoder/embedding_encoder:0, (7709, 128), /device:GPU:0\n",
            "  embeddings/decoder/embedding_decoder:0, (17191, 128), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 17191), /device:GPU:0\n",
            "WARNING:tensorflow:Entity <function get_infer_iterator.<locals>.<lambda> at 0x7f2b3c977ae8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:15.744102 139824990775168 ag_logging.py:146] Entity <function get_infer_iterator.<locals>.<lambda> at 0x7f2b3c977ae8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_infer_iterator.<locals>.<lambda> at 0x7f2b1b1199d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:15.753349 139824990775168 ag_logging.py:146] Entity <function get_infer_iterator.<locals>.<lambda> at 0x7f2b1b1199d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_infer_iterator.<locals>.<lambda> at 0x7f2ab2ed2f28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 06:26:15.761023 139824990775168 ag_logging.py:146] Entity <function get_infer_iterator.<locals>.<lambda> at 0x7f2ab2ed2f28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "# Creating infer graph ...\n",
            "# Build a basic encoder\n",
            "  num_layers = 2, num_residual_layers=0\n",
            "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000\n",
            "WARNING:tensorflow:From /content/nmt/nmt/attention_model.py:193: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "W0129 06:26:16.259596 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/attention_model.py:193: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "# Trainable variables\n",
            "Format: <name>, <shape>, <(soft) device placement>\n",
            "  embeddings/encoder/embedding_encoder:0, (7709, 128), /device:GPU:0\n",
            "  embeddings/decoder/embedding_decoder:0, (17191, 128), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 17191), \n",
            "# log_file=/tmp/nmt_attention_model/log_1611901576\n",
            "WARNING:tensorflow:From /content/nmt/nmt/utils/misc_utils.py:142: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0129 06:26:16.280408 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/utils/misc_utils.py:142: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "2021-01-29 06:26:16.281034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.281621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-01-29 06:26:16.281707: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-01-29 06:26:16.281744: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-01-29 06:26:16.281771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-01-29 06:26:16.281806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-01-29 06:26:16.281832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-01-29 06:26:16.281855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-01-29 06:26:16.281879: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-01-29 06:26:16.281970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.282652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.283222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-01-29 06:26:16.283273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-01-29 06:26:16.283297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-01-29 06:26:16.283312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-01-29 06:26:16.283480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.284026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.284563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2021-01-29 06:26:16.285030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.285567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-01-29 06:26:16.285623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-01-29 06:26:16.285655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-01-29 06:26:16.285681: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-01-29 06:26:16.285715: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-01-29 06:26:16.285743: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-01-29 06:26:16.285766: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-01-29 06:26:16.285789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-01-29 06:26:16.285861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.286419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.286910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-01-29 06:26:16.286944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-01-29 06:26:16.286960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-01-29 06:26:16.286986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-01-29 06:26:16.287079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.287646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.288146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2021-01-29 06:26:16.288611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.289130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-01-29 06:26:16.289188: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-01-29 06:26:16.289225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-01-29 06:26:16.289250: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-01-29 06:26:16.289285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-01-29 06:26:16.289311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-01-29 06:26:16.289350: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-01-29 06:26:16.289373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-01-29 06:26:16.289445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.289996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.290521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-01-29 06:26:16.290554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-01-29 06:26:16.290576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-01-29 06:26:16.290591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-01-29 06:26:16.290693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.291304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 06:26:16.291832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:628: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "W0129 06:26:16.292058 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:628: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:629: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
            "\n",
            "W0129 06:26:23.726366 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:629: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
            "\n",
            "  created train model with fresh parameters, time 7.53s\n",
            "WARNING:tensorflow:From /content/nmt/nmt/train.py:500: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0129 06:26:23.841840 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/train.py:500: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "  created infer model with fresh parameters, time 0.09s\n",
            "  # 831\n",
            "2021-01-29 06:26:24.508847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "    src: Với những điều đó , sai lầm là bất khả kháng .\n",
            "    ref: Given all of that , mistakes are inevitable .\n",
            "    nmt: microbial shaky vein medals medals scanned scanned intend intend intend intend intend dictionary All comic comic comic tolerant tolerant tolerant tolerant existing existing existing\n",
            "  created eval model with fresh parameters, time 0.10s\n",
            "  eval dev: perplexity 17192.35, time 1s, Fri Jan 29 06:26:27 2021.\n",
            "WARNING:tensorflow:From /content/nmt/nmt/utils/misc_utils.py:134: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "W0129 06:26:27.877392 139824990775168 module_wrapper.py:139] From /content/nmt/nmt/utils/misc_utils.py:134: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "  eval test: perplexity 17191.83, time 1s, Fri Jan 29 06:26:29 2021.\n",
            "  created infer model with fresh parameters, time 0.08s\n",
            "# Start step 0, lr 1, Fri Jan 29 06:26:29 2021\n",
            "# Init train iterator, skipping 0 elements\n",
            "  step 100 lr 1 step-time 0.28s wps 19.68K ppl 35120.75 gN 96.67 bleu 0.00, Fri Jan 29 06:26:57 2021\n",
            "  step 200 lr 1 step-time 0.21s wps 26.20K ppl 1122.99 gN 16.57 bleu 0.00, Fri Jan 29 06:27:19 2021\n",
            "  step 300 lr 1 step-time 0.14s wps 41.84K ppl 651.00 gN 8.46 bleu 0.00, Fri Jan 29 06:27:32 2021\n",
            "  step 400 lr 1 step-time 0.13s wps 43.17K ppl 480.77 gN 6.82 bleu 0.00, Fri Jan 29 06:27:45 2021\n",
            "  step 500 lr 1 step-time 0.13s wps 44.77K ppl 375.63 gN 5.39 bleu 0.00, Fri Jan 29 06:27:58 2021\n",
            "  step 600 lr 1 step-time 0.13s wps 42.68K ppl 301.46 gN 4.77 bleu 0.00, Fri Jan 29 06:28:11 2021\n",
            "  step 700 lr 1 step-time 0.14s wps 42.10K ppl 266.62 gN 4.91 bleu 0.00, Fri Jan 29 06:28:24 2021\n",
            "  step 800 lr 1 step-time 0.13s wps 42.58K ppl 233.46 gN 4.74 bleu 0.00, Fri Jan 29 06:28:37 2021\n",
            "  step 900 lr 1 step-time 0.13s wps 44.15K ppl 218.17 gN 4.56 bleu 0.00, Fri Jan 29 06:28:50 2021\n",
            "  step 1000 lr 1 step-time 0.13s wps 41.26K ppl 197.69 gN 4.09 bleu 0.00, Fri Jan 29 06:29:04 2021\n",
            "# Save eval, global step 1000\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-1000\n",
            "I0129 06:29:04.494125 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-1000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-1000, time 0.08s\n",
            "  # 693\n",
            "    src: Và tôi thuộc làu mọi thứ .\n",
            "    ref: And I memorized everything .\n",
            "    nmt: And I &apos;m going to do .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-1000\n",
            "I0129 06:29:04.606809 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-1000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-1000, time 0.08s\n",
            "  eval dev: perplexity 137.98, time 1s, Fri Jan 29 06:29:06 2021.\n",
            "  eval test: perplexity 156.22, time 1s, Fri Jan 29 06:29:07 2021.\n",
            "# Finished an epoch, step 1043. Perform external evaluation\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-1000\n",
            "I0129 06:29:13.208374 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-1000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-1000, time 0.05s\n",
            "  # 339\n",
            "    src: Tôi bắt đầu suy nghĩ , làm sao mà chuyện này có thể xảy ra được ?\n",
            "    ref: I start thinking , well , how did that happen ?\n",
            "    nmt: And I &apos;m going to be the <unk> of the world .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-1000\n",
            "I0129 06:29:13.305033 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-1000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-1000, time 0.05s\n",
            "# External evaluation, global step 1000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 8s, Fri Jan 29 06:29:21 2021.\n",
            "  bleu dev: 1.2\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 1000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 7s, Fri Jan 29 06:29:29 2021.\n",
            "  bleu test: 1.0\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 1100 lr 1 step-time 0.22s wps 24.87K ppl 187.93 gN 4.13 bleu 1.17, Fri Jan 29 06:29:46 2021\n",
            "  step 1200 lr 1 step-time 0.26s wps 21.57K ppl 172.00 gN 3.80 bleu 1.17, Fri Jan 29 06:30:12 2021\n",
            "  step 1300 lr 1 step-time 0.13s wps 44.22K ppl 163.05 gN 3.60 bleu 1.17, Fri Jan 29 06:30:25 2021\n",
            "  step 1400 lr 1 step-time 0.13s wps 42.38K ppl 157.64 gN 3.74 bleu 1.17, Fri Jan 29 06:30:38 2021\n",
            "  step 1500 lr 1 step-time 0.13s wps 43.00K ppl 150.78 gN 3.73 bleu 1.17, Fri Jan 29 06:30:51 2021\n",
            "  step 1600 lr 1 step-time 0.13s wps 44.33K ppl 142.49 gN 3.66 bleu 1.17, Fri Jan 29 06:31:04 2021\n",
            "  step 1700 lr 1 step-time 0.13s wps 43.28K ppl 135.61 gN 3.51 bleu 1.17, Fri Jan 29 06:31:17 2021\n",
            "  step 1800 lr 1 step-time 0.14s wps 40.94K ppl 128.56 gN 3.66 bleu 1.17, Fri Jan 29 06:31:31 2021\n",
            "  step 1900 lr 1 step-time 0.12s wps 44.43K ppl 124.36 gN 3.99 bleu 1.17, Fri Jan 29 06:31:43 2021\n",
            "  step 2000 lr 1 step-time 0.13s wps 43.18K ppl 116.13 gN 3.68 bleu 1.17, Fri Jan 29 06:31:56 2021\n",
            "# Save eval, global step 2000\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-2000\n",
            "I0129 06:31:56.847253 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-2000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-2000, time 0.06s\n",
            "  # 1174\n",
            "    src: Điều này đang diễn ra ngày càng nhanh và đó là lí do tại sao tôi thấy nó khá mơ hồ .\n",
            "    ref: The thing is becoming faster and that &apos;s why I think it seems so confusing .\n",
            "    nmt: It &apos;s not the <unk> , and I &apos;m going to be <unk> .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-2000\n",
            "I0129 06:31:56.964855 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-2000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-2000, time 0.06s\n",
            "  eval dev: perplexity 84.67, time 1s, Fri Jan 29 06:31:58 2021.\n",
            "  eval test: perplexity 95.88, time 0s, Fri Jan 29 06:31:59 2021.\n",
            "# Finished an epoch, step 2086. Perform external evaluation\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-2000\n",
            "I0129 06:32:11.203006 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-2000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-2000, time 0.06s\n",
            "  # 1263\n",
            "    src: 1 chú chuột opossum sao ? Thử kiểm tra xem . Còn sống ? Đúng vậy .\n",
            "    ref: Opossum ? Check . Living ? Yep .\n",
            "    nmt: <unk> <unk> <unk> . <unk> <unk> . <unk> <unk> .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-2000\n",
            "I0129 06:32:11.300391 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-2000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-2000, time 0.06s\n",
            "# External evaluation, global step 2000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 13s, Fri Jan 29 06:32:24 2021.\n",
            "  bleu dev: 1.5\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 2000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 17s, Fri Jan 29 06:32:42 2021.\n",
            "  bleu test: 1.1\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 2100 lr 1 step-time 0.16s wps 34.84K ppl 111.29 gN 4.06 bleu 1.48, Fri Jan 29 06:32:46 2021\n",
            "  step 2200 lr 1 step-time 0.26s wps 22.00K ppl 105.65 gN 4.09 bleu 1.48, Fri Jan 29 06:33:12 2021\n",
            "  step 2300 lr 1 step-time 0.19s wps 29.63K ppl 96.77 gN 3.96 bleu 1.48, Fri Jan 29 06:33:31 2021\n",
            "  step 2400 lr 1 step-time 0.12s wps 44.42K ppl 92.42 gN 4.07 bleu 1.48, Fri Jan 29 06:33:43 2021\n",
            "  step 2500 lr 1 step-time 0.13s wps 43.65K ppl 87.15 gN 4.28 bleu 1.48, Fri Jan 29 06:33:56 2021\n",
            "  step 2600 lr 1 step-time 0.13s wps 44.26K ppl 82.91 gN 4.25 bleu 1.48, Fri Jan 29 06:34:09 2021\n",
            "  step 2700 lr 1 step-time 0.13s wps 44.15K ppl 81.13 gN 4.60 bleu 1.48, Fri Jan 29 06:34:22 2021\n",
            "  step 2800 lr 1 step-time 0.13s wps 42.47K ppl 73.01 gN 4.53 bleu 1.48, Fri Jan 29 06:34:35 2021\n",
            "  step 2900 lr 1 step-time 0.13s wps 44.91K ppl 70.08 gN 4.73 bleu 1.48, Fri Jan 29 06:34:47 2021\n",
            "  step 3000 lr 1 step-time 0.13s wps 43.09K ppl 65.95 gN 4.62 bleu 1.48, Fri Jan 29 06:35:00 2021\n",
            "# Save eval, global step 3000\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-3000\n",
            "I0129 06:35:01.187238 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-3000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-3000, time 0.06s\n",
            "  # 780\n",
            "    src: Và ngay cả khi anh ta đang đi bộ ra khỏi cửa , anh ta cũng kiều như đang chỉ về cổ .\n",
            "    ref: And even as he was walking out the door , he was still sort of pointing to his throat .\n",
            "    nmt: And so you &apos;re going to be going to be going to be the same way .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-3000\n",
            "I0129 06:35:01.301217 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-3000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-3000, time 0.06s\n",
            "  eval dev: perplexity 48.08, time 1s, Fri Jan 29 06:35:02 2021.\n",
            "  eval test: perplexity 53.19, time 1s, Fri Jan 29 06:35:03 2021.\n",
            "  step 3100 lr 1 step-time 0.13s wps 42.54K ppl 61.79 gN 4.74 bleu 1.48, Fri Jan 29 06:35:16 2021\n",
            "# Finished an epoch, step 3129. Perform external evaluation\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-3000\n",
            "I0129 06:35:20.946383 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-3000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-3000, time 0.06s\n",
            "  # 1282\n",
            "    src: Và những gì ở phía dưới được chúng ta gọi là bộ máy quan liêu .\n",
            "    ref: And what &apos;s under that is what we call bureaucracy .\n",
            "    nmt: And what we call is that we call our own brain .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-3000\n",
            "I0129 06:35:21.044598 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-3000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-3000, time 0.06s\n",
            "# External evaluation, global step 3000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 13s, Fri Jan 29 06:35:34 2021.\n",
            "  bleu dev: 4.5\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 3000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 13s, Fri Jan 29 06:35:47 2021.\n",
            "  bleu test: 4.1\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 3200 lr 1 step-time 0.22s wps 24.56K ppl 56.49 gN 4.83 bleu 4.45, Fri Jan 29 06:36:06 2021\n",
            "  step 3300 lr 1 step-time 0.24s wps 22.82K ppl 52.21 gN 4.78 bleu 4.45, Fri Jan 29 06:36:30 2021\n",
            "  step 3400 lr 1 step-time 0.14s wps 41.00K ppl 52.14 gN 5.01 bleu 4.45, Fri Jan 29 06:36:44 2021\n",
            "  step 3500 lr 1 step-time 0.13s wps 44.55K ppl 48.04 gN 4.84 bleu 4.45, Fri Jan 29 06:36:56 2021\n",
            "  step 3600 lr 1 step-time 0.13s wps 44.28K ppl 44.90 gN 4.71 bleu 4.45, Fri Jan 29 06:37:09 2021\n",
            "  step 3700 lr 1 step-time 0.13s wps 44.12K ppl 45.31 gN 4.93 bleu 4.45, Fri Jan 29 06:37:22 2021\n",
            "  step 3800 lr 1 step-time 0.13s wps 44.90K ppl 43.94 gN 5.13 bleu 4.45, Fri Jan 29 06:37:35 2021\n",
            "  step 3900 lr 1 step-time 0.13s wps 43.65K ppl 41.55 gN 4.93 bleu 4.45, Fri Jan 29 06:37:48 2021\n",
            "  step 4000 lr 1 step-time 0.13s wps 44.00K ppl 39.91 gN 4.85 bleu 4.45, Fri Jan 29 06:38:00 2021\n",
            "# Save eval, global step 4000\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-4000\n",
            "I0129 06:38:00.914921 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-4000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-4000, time 0.06s\n",
            "  # 701\n",
            "    src: Lúc đó tôi được giao nhiệm vụ ở khoa tim trực ban khoa tim .\n",
            "    ref: At the time I was assigned to the cardiology service on a cardiology rotation .\n",
            "    nmt: I was <unk> in the computer of the computer .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-4000\n",
            "I0129 06:38:01.020904 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-4000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-4000, time 0.06s\n",
            "  eval dev: perplexity 32.26, time 1s, Fri Jan 29 06:38:02 2021.\n",
            "  eval test: perplexity 33.36, time 0s, Fri Jan 29 06:38:03 2021.\n",
            "  step 4100 lr 1 step-time 0.13s wps 43.88K ppl 38.18 gN 4.89 bleu 4.45, Fri Jan 29 06:38:16 2021\n",
            "# Finished an epoch, step 4172. Perform external evaluation\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-4000\n",
            "I0129 06:38:25.368735 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-4000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-4000, time 0.06s\n",
            "  # 1052\n",
            "    src: Và do đó , tôi nghĩ bức ảnh này có ý nghĩa của nó nếu mọi người nhìn vào nó với góc độ vài tỉ năm .\n",
            "    ref: So I think this picture makes sense if you look at it a few billion years at a time .\n",
            "    nmt: And so , I think , I think that this is what &apos;s going to be in the middle of the past .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-4000\n",
            "I0129 06:38:25.483500 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-4000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-4000, time 0.06s\n",
            "# External evaluation, global step 4000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 13s, Fri Jan 29 06:38:39 2021.\n",
            "  bleu dev: 6.9\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 4000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 14s, Fri Jan 29 06:38:54 2021.\n",
            "  bleu test: 6.3\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 4200 lr 1 step-time 0.17s wps 32.83K ppl 36.12 gN 5.24 bleu 6.85, Fri Jan 29 06:39:02 2021\n",
            "  step 4300 lr 1 step-time 0.27s wps 21.27K ppl 35.14 gN 5.02 bleu 6.85, Fri Jan 29 06:39:28 2021\n",
            "  step 4400 lr 1 step-time 0.16s wps 34.24K ppl 34.38 gN 5.17 bleu 6.85, Fri Jan 29 06:39:45 2021\n",
            "  step 4500 lr 1 step-time 0.13s wps 44.06K ppl 33.26 gN 5.13 bleu 6.85, Fri Jan 29 06:39:58 2021\n",
            "  step 4600 lr 1 step-time 0.12s wps 45.97K ppl 32.79 gN 5.26 bleu 6.85, Fri Jan 29 06:40:10 2021\n",
            "  step 4700 lr 1 step-time 0.13s wps 44.24K ppl 31.87 gN 5.06 bleu 6.85, Fri Jan 29 06:40:23 2021\n",
            "  step 4800 lr 1 step-time 0.12s wps 45.00K ppl 31.84 gN 5.15 bleu 6.85, Fri Jan 29 06:40:35 2021\n",
            "  step 4900 lr 1 step-time 0.13s wps 44.01K ppl 30.45 gN 5.08 bleu 6.85, Fri Jan 29 06:40:48 2021\n",
            "  step 5000 lr 1 step-time 0.12s wps 45.08K ppl 30.23 gN 5.15 bleu 6.85, Fri Jan 29 06:41:00 2021\n",
            "# Save eval, global step 5000\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-5000\n",
            "I0129 06:41:01.122953 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-5000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-5000, time 0.07s\n",
            "  # 1551\n",
            "    src: Và liệu chúng ta có còn tin tưởng , một cách mù quáng vào bất kỳ chính phủ tương lai nào , một chính phủ mà chúng ta có thể có trong 50 năm tới ?\n",
            "    ref: And do we trust , do we blindly trust , any future government , a government we might have 50 years from now ?\n",
            "    nmt: And what do we believe , a bad way to be able to be in any single state of the brain , a government that we can be in 50 years ?\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-5000\n",
            "I0129 06:41:01.282915 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-5000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-5000, time 0.06s\n",
            "  eval dev: perplexity 25.81, time 1s, Fri Jan 29 06:41:02 2021.\n",
            "  eval test: perplexity 25.29, time 1s, Fri Jan 29 06:41:03 2021.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-5000\n",
            "I0129 06:41:03.854479 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-5000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-5000, time 0.07s\n",
            "  # 1034\n",
            "    src: Đó là tất cả những gì có thể xảy ra .\n",
            "    ref: That &apos;s about all it can do .\n",
            "    nmt: That &apos;s all the way that there can happen .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-5000\n",
            "I0129 06:41:03.972691 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-5000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-5000, time 0.06s\n",
            "# External evaluation, global step 5000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 14s, Fri Jan 29 06:41:18 2021.\n",
            "  bleu dev: 9.3\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 5000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 16s, Fri Jan 29 06:41:35 2021.\n",
            "  bleu test: 9.8\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 5100 lr 1 step-time 0.13s wps 43.24K ppl 29.98 gN 5.29 bleu 9.34, Fri Jan 29 06:41:48 2021\n",
            "  step 5200 lr 1 step-time 0.13s wps 43.59K ppl 28.91 gN 5.23 bleu 9.34, Fri Jan 29 06:42:01 2021\n",
            "# Finished an epoch, step 5215. Perform external evaluation\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-5000\n",
            "I0129 06:42:03.503981 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-5000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-5000, time 0.06s\n",
            "  # 1401\n",
            "    src: Thứ hai , kết hợp các bức ảnh nên có cùng một kiểu ánh sáng .\n",
            "    ref: Secondly , photos combined should have the same type of light .\n",
            "    nmt: The second , the images that have come together there are a light light light .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-5000\n",
            "I0129 06:42:03.616430 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-5000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-5000, time 0.06s\n",
            "# External evaluation, global step 5000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 14s, Fri Jan 29 06:42:18 2021.\n",
            "  bleu dev: 9.3\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 5000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 15s, Fri Jan 29 06:42:33 2021.\n",
            "  bleu test: 9.8\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 5300 lr 1 step-time 0.25s wps 22.06K ppl 26.72 gN 5.42 bleu 9.34, Fri Jan 29 06:42:56 2021\n",
            "  step 5400 lr 1 step-time 0.22s wps 25.21K ppl 27.05 gN 5.36 bleu 9.34, Fri Jan 29 06:43:19 2021\n",
            "  step 5500 lr 1 step-time 0.13s wps 43.92K ppl 27.04 gN 5.29 bleu 9.34, Fri Jan 29 06:43:32 2021\n",
            "  step 5600 lr 1 step-time 0.13s wps 41.66K ppl 26.17 gN 5.28 bleu 9.34, Fri Jan 29 06:43:45 2021\n",
            "  step 5700 lr 1 step-time 0.14s wps 41.57K ppl 26.17 gN 5.35 bleu 9.34, Fri Jan 29 06:43:59 2021\n",
            "  step 5800 lr 1 step-time 0.13s wps 43.82K ppl 25.10 gN 5.13 bleu 9.34, Fri Jan 29 06:44:11 2021\n",
            "  step 5900 lr 1 step-time 0.13s wps 44.07K ppl 25.61 gN 5.29 bleu 9.34, Fri Jan 29 06:44:24 2021\n",
            "  step 6000 lr 1 step-time 0.13s wps 42.13K ppl 24.82 gN 5.17 bleu 9.34, Fri Jan 29 06:44:37 2021\n",
            "# Save eval, global step 6000\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "W0129 06:44:37.989755 139824990775168 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-6000\n",
            "I0129 06:44:38.155698 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-6000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-6000, time 0.07s\n",
            "  # 973\n",
            "    src: Ý tưởng nhốt đom đóm trong lọ thuỷ tinh , không biết tại sao , luôn vô cùng thú vị đối với tôi .\n",
            "    ref: This idea of fireflies in a jar , for some reason , was always really exciting to me .\n",
            "    nmt: The idea of <unk> in the circle of the circle , no idea why , always , it &apos;s always interesting .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-6000\n",
            "I0129 06:44:38.295576 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-6000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-6000, time 0.06s\n",
            "  eval dev: perplexity 20.60, time 1s, Fri Jan 29 06:44:39 2021.\n",
            "  eval test: perplexity 20.37, time 1s, Fri Jan 29 06:44:40 2021.\n",
            "  step 6100 lr 1 step-time 0.13s wps 44.76K ppl 25.46 gN 5.49 bleu 9.34, Fri Jan 29 06:44:53 2021\n",
            "  step 6200 lr 1 step-time 0.13s wps 43.16K ppl 24.30 gN 5.28 bleu 9.34, Fri Jan 29 06:45:06 2021\n",
            "# Finished an epoch, step 6258. Perform external evaluation\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-6000\n",
            "I0129 06:45:13.474140 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-6000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-6000, time 0.06s\n",
            "  # 847\n",
            "    src: Tên tôi là Brian Goldman .\n",
            "    ref: My name is Brian Goldman .\n",
            "    nmt: My name is Brian Clinton .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-6000\n",
            "I0129 06:45:13.558902 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-6000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-6000, time 0.06s\n",
            "# External evaluation, global step 6000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 11s, Fri Jan 29 06:45:24 2021.\n",
            "  bleu dev: 11.6\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 6000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 10s, Fri Jan 29 06:45:35 2021.\n",
            "  bleu test: 11.8\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 6300 lr 1 step-time 0.19s wps 28.93K ppl 23.91 gN 5.57 bleu 11.61, Fri Jan 29 06:45:47 2021\n",
            "  step 6400 lr 1 step-time 0.25s wps 21.87K ppl 22.39 gN 5.21 bleu 11.61, Fri Jan 29 06:46:13 2021\n",
            "  step 6500 lr 1 step-time 0.15s wps 37.41K ppl 22.38 gN 5.27 bleu 11.61, Fri Jan 29 06:46:28 2021\n",
            "  step 6600 lr 1 step-time 0.13s wps 43.55K ppl 22.11 gN 5.33 bleu 11.61, Fri Jan 29 06:46:40 2021\n",
            "  step 6700 lr 1 step-time 0.13s wps 42.94K ppl 23.05 gN 5.43 bleu 11.61, Fri Jan 29 06:46:54 2021\n",
            "  step 6800 lr 1 step-time 0.13s wps 42.90K ppl 21.78 gN 5.39 bleu 11.61, Fri Jan 29 06:47:07 2021\n",
            "  step 6900 lr 1 step-time 0.12s wps 46.02K ppl 22.21 gN 5.37 bleu 11.61, Fri Jan 29 06:47:19 2021\n",
            "  step 7000 lr 1 step-time 0.13s wps 44.72K ppl 21.94 gN 5.35 bleu 11.61, Fri Jan 29 06:47:32 2021\n",
            "# Save eval, global step 7000\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-7000\n",
            "I0129 06:47:32.346033 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-7000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-7000, time 0.06s\n",
            "  # 1132\n",
            "    src: Bởi có hàng tỉ bóng bán dẫn .\n",
            "    ref: There are billions of them .\n",
            "    nmt: Because there are a billion of these <unk> .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-7000\n",
            "I0129 06:47:32.439844 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-7000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-7000, time 0.06s\n",
            "  eval dev: perplexity 18.90, time 1s, Fri Jan 29 06:47:33 2021.\n",
            "  eval test: perplexity 18.51, time 0s, Fri Jan 29 06:47:34 2021.\n",
            "  step 7100 lr 1 step-time 0.13s wps 43.39K ppl 21.36 gN 5.21 bleu 11.61, Fri Jan 29 06:47:47 2021\n",
            "  step 7200 lr 1 step-time 0.14s wps 41.18K ppl 21.52 gN 5.32 bleu 11.61, Fri Jan 29 06:48:01 2021\n",
            "  step 7300 lr 1 step-time 0.13s wps 42.97K ppl 21.44 gN 5.43 bleu 11.61, Fri Jan 29 06:48:14 2021\n",
            "# Finished an epoch, step 7301. Perform external evaluation\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-7000\n",
            "I0129 06:48:14.484774 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-7000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-7000, time 0.06s\n",
            "  # 342\n",
            "    src: Và có một đêm , tôi thức rất khuya và bắt đầu nghĩ lạ nhỉ , nếu quan toà có thể làm vậy thì hẳn là họ có quyền lực vô biên\n",
            "    ref: And I was up too late one night and I starting thinking , well gosh , if the judge can turn you into something that you &apos;re not , the judge must have magic power .\n",
            "    nmt: And there &apos;s a night , I &apos;m a very long time and started thinking about it , if the building can do it , it &apos;s a lot of <unk> .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-7000\n",
            "I0129 06:48:14.617547 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-7000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-7000, time 0.06s\n",
            "# External evaluation, global step 7000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 10s, Fri Jan 29 06:48:24 2021.\n",
            "  bleu dev: 11.2\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 7000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 10s, Fri Jan 29 06:48:35 2021.\n",
            "  bleu test: 12.0\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 7400 lr 1 step-time 0.28s wps 20.30K ppl 19.90 gN 5.84 bleu 11.61, Fri Jan 29 06:49:03 2021\n",
            "  step 7500 lr 1 step-time 0.21s wps 26.93K ppl 20.02 gN 5.30 bleu 11.61, Fri Jan 29 06:49:23 2021\n",
            "  step 7600 lr 1 step-time 0.13s wps 44.20K ppl 19.68 gN 5.25 bleu 11.61, Fri Jan 29 06:49:36 2021\n",
            "  step 7700 lr 1 step-time 0.13s wps 44.54K ppl 19.97 gN 5.45 bleu 11.61, Fri Jan 29 06:49:49 2021\n",
            "  step 7800 lr 1 step-time 0.13s wps 43.01K ppl 19.84 gN 5.39 bleu 11.61, Fri Jan 29 06:50:02 2021\n",
            "  step 7900 lr 1 step-time 0.14s wps 40.82K ppl 19.36 gN 5.26 bleu 11.61, Fri Jan 29 06:50:16 2021\n",
            "  step 8000 lr 1 step-time 0.13s wps 44.66K ppl 19.94 gN 5.42 bleu 11.61, Fri Jan 29 06:50:28 2021\n",
            "# Save eval, global step 8000\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-8000\n",
            "I0129 06:50:28.796890 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-8000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-8000, time 0.06s\n",
            "  # 1543\n",
            "    src: Vậy tại sao tôi phải lo lắng ?\n",
            "    ref: Why should I worry ?\n",
            "    nmt: So why am I concerned ?\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-8000\n",
            "I0129 06:50:28.888198 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-8000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-8000, time 0.07s\n",
            "  eval dev: perplexity 21.22, time 1s, Fri Jan 29 06:50:30 2021.\n",
            "  eval test: perplexity 19.72, time 1s, Fri Jan 29 06:50:31 2021.\n",
            "  step 8100 lr 1 step-time 0.13s wps 42.34K ppl 19.66 gN 5.48 bleu 11.61, Fri Jan 29 06:50:44 2021\n",
            "  step 8200 lr 1 step-time 0.13s wps 44.26K ppl 19.20 gN 5.33 bleu 11.61, Fri Jan 29 06:50:57 2021\n",
            "  step 8300 lr 1 step-time 0.12s wps 44.86K ppl 19.08 gN 5.26 bleu 11.61, Fri Jan 29 06:51:09 2021\n",
            "# Finished an epoch, step 8344. Perform external evaluation\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-8000\n",
            "I0129 06:51:15.937508 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-8000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-8000, time 0.06s\n",
            "  # 378\n",
            "    src: Khoảng 10 phút sau khi quan toà tuyên bố nghỉ giải lao .\n",
            "    ref: About 10 minutes later the judge said we would take a break .\n",
            "    nmt: About 10 minutes after writing their father .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-8000\n",
            "I0129 06:51:16.030275 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-8000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-8000, time 0.06s\n",
            "# External evaluation, global step 8000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 9s, Fri Jan 29 06:51:25 2021.\n",
            "  bleu dev: 13.2\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 8000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 8s, Fri Jan 29 06:51:34 2021.\n",
            "  bleu test: 14.7\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 8400 lr 1 step-time 0.21s wps 27.01K ppl 18.55 gN 5.78 bleu 13.22, Fri Jan 29 06:51:49 2021\n",
            "  step 8500 lr 1 step-time 0.27s wps 20.91K ppl 18.14 gN 5.49 bleu 13.22, Fri Jan 29 06:52:16 2021\n",
            "  step 8600 lr 1 step-time 0.13s wps 42.77K ppl 17.84 gN 5.29 bleu 13.22, Fri Jan 29 06:52:29 2021\n",
            "  step 8700 lr 1 step-time 0.13s wps 43.81K ppl 17.87 gN 5.34 bleu 13.22, Fri Jan 29 06:52:41 2021\n",
            "  step 8800 lr 1 step-time 0.13s wps 43.94K ppl 18.17 gN 5.36 bleu 13.22, Fri Jan 29 06:52:54 2021\n",
            "  step 8900 lr 1 step-time 0.14s wps 41.02K ppl 18.01 gN 5.36 bleu 13.22, Fri Jan 29 06:53:08 2021\n",
            "  step 9000 lr 1 step-time 0.13s wps 42.85K ppl 17.99 gN 5.32 bleu 13.22, Fri Jan 29 06:53:21 2021\n",
            "# Save eval, global step 9000\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-9000\n",
            "I0129 06:53:21.822140 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-9000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-9000, time 0.06s\n",
            "  # 671\n",
            "    src: Thế bạn có biết người ta gọi một cầu thủ với chỉ số 400 là gì không ?\n",
            "    ref: Do you know what they call a 400 baseball hitter ?\n",
            "    nmt: So you know people called a player with 400 years ?\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-9000\n",
            "I0129 06:53:21.924725 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-9000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-9000, time 0.07s\n",
            "  eval dev: perplexity 18.58, time 1s, Fri Jan 29 06:53:23 2021.\n",
            "  eval test: perplexity 16.90, time 1s, Fri Jan 29 06:53:24 2021.\n",
            "  step 9100 lr 1 step-time 0.13s wps 42.54K ppl 17.67 gN 5.30 bleu 13.22, Fri Jan 29 06:53:37 2021\n",
            "  step 9200 lr 1 step-time 0.13s wps 45.47K ppl 17.68 gN 5.27 bleu 13.22, Fri Jan 29 06:53:50 2021\n",
            "  step 9300 lr 1 step-time 0.12s wps 45.34K ppl 17.95 gN 5.29 bleu 13.22, Fri Jan 29 06:54:02 2021\n",
            "# Finished an epoch, step 9387. Perform external evaluation\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-9000\n",
            "I0129 06:54:14.232768 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-9000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-9000, time 0.07s\n",
            "  # 68\n",
            "    src: Thế là tôi nói về nạn thanh niên thất nghiệp và giáo dục , và về những người bị xã hội cách li , bị mất quyền lợi và bỏ rơi .\n",
            "    ref: So I spoke out on youth unemployment and education and the neglect of the marginalized and the disenfranchised .\n",
            "    nmt: So I was talking about my mom , and education , and about people who are social , lost power and left .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-9000\n",
            "I0129 06:54:14.367088 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-9000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-9000, time 0.06s\n",
            "# External evaluation, global step 9000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 9s, Fri Jan 29 06:54:24 2021.\n",
            "  bleu dev: 14.2\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 9000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 10s, Fri Jan 29 06:54:34 2021.\n",
            "  bleu test: 16.4\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 9400 lr 1 step-time 0.16s wps 34.22K ppl 17.58 gN 5.59 bleu 14.17, Fri Jan 29 06:54:38 2021\n",
            "  step 9500 lr 1 step-time 0.27s wps 20.78K ppl 16.53 gN 5.41 bleu 14.17, Fri Jan 29 06:55:05 2021\n",
            "  step 9600 lr 1 step-time 0.19s wps 29.34K ppl 16.74 gN 5.26 bleu 14.17, Fri Jan 29 06:55:25 2021\n",
            "  step 9700 lr 1 step-time 0.12s wps 45.80K ppl 16.79 gN 5.51 bleu 14.17, Fri Jan 29 06:55:37 2021\n",
            "  step 9800 lr 1 step-time 0.13s wps 44.70K ppl 16.60 gN 5.49 bleu 14.17, Fri Jan 29 06:55:49 2021\n",
            "  step 9900 lr 1 step-time 0.14s wps 41.33K ppl 17.09 gN 5.42 bleu 14.17, Fri Jan 29 06:56:03 2021\n",
            "  step 10000 lr 1 step-time 0.13s wps 42.49K ppl 16.69 gN 5.31 bleu 14.17, Fri Jan 29 06:56:16 2021\n",
            "# Save eval, global step 10000\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-10000\n",
            "I0129 06:56:17.198975 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-10000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-10000, time 0.06s\n",
            "  # 1037\n",
            "    src: Chúng ta đang ở trên đường thẳng này trong quá trình chuyển giao giữa trạng thái thế giới từng tồn tại tới trạng thái mới mà thế giới đang tồn tại .\n",
            "    ref: We &apos;re sort of on this line in a transition from the way the world used to be to some new way that the world is .\n",
            "    nmt: We &apos;re in this straight line in the middle of the world between the world , which is in the world .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-10000\n",
            "I0129 06:56:17.319752 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-10000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-10000, time 0.06s\n",
            "  eval dev: perplexity 15.46, time 1s, Fri Jan 29 06:56:18 2021.\n",
            "  eval test: perplexity 14.38, time 1s, Fri Jan 29 06:56:19 2021.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-10000\n",
            "I0129 06:56:19.942836 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-10000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-10000, time 0.06s\n",
            "  # 668\n",
            "    src: Cứ 10 lần thì có 3 lần như vậy .\n",
            "    ref: Three times out of 10 .\n",
            "    nmt: And you &apos;ve got 10 times that it &apos;s two times .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-10000\n",
            "I0129 06:56:20.042363 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-10000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-10000, time 0.06s\n",
            "# External evaluation, global step 10000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 8s, Fri Jan 29 06:56:28 2021.\n",
            "  bleu dev: 14.8\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 10000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 8s, Fri Jan 29 06:56:37 2021.\n",
            "  bleu test: 16.7\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 10100 lr 1 step-time 0.13s wps 42.09K ppl 16.65 gN 5.39 bleu 14.77, Fri Jan 29 06:56:51 2021\n",
            "  step 10200 lr 1 step-time 0.13s wps 44.66K ppl 16.43 gN 5.15 bleu 14.77, Fri Jan 29 06:57:04 2021\n",
            "  step 10300 lr 1 step-time 0.13s wps 41.73K ppl 16.25 gN 5.21 bleu 14.77, Fri Jan 29 06:57:17 2021\n",
            "  step 10400 lr 1 step-time 0.12s wps 45.17K ppl 16.58 gN 5.28 bleu 14.77, Fri Jan 29 06:57:29 2021\n",
            "# Finished an epoch, step 10430. Perform external evaluation\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-10000\n",
            "I0129 06:57:33.616347 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-10000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-10000, time 0.06s\n",
            "  # 580\n",
            "    src: Và nếu người làm công của bạn không thể phân biệt. thì người tiêu dùng của bạn cũng vậy .\n",
            "    ref: And if your employees can &apos;t tell them apart , neither can your consumers . &quot;\n",
            "    nmt: And if you do your work can &apos;t <unk> your <unk> .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-10000\n",
            "I0129 06:57:33.722240 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-10000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-10000, time 0.05s\n",
            "# External evaluation, global step 10000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 9s, Fri Jan 29 06:57:42 2021.\n",
            "  bleu dev: 14.8\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 10000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 8s, Fri Jan 29 06:57:51 2021.\n",
            "  bleu test: 16.7\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 10500 lr 1 step-time 0.23s wps 24.23K ppl 15.57 gN 5.55 bleu 14.77, Fri Jan 29 06:58:11 2021\n",
            "  step 10600 lr 1 step-time 0.24s wps 22.89K ppl 15.69 gN 5.35 bleu 14.77, Fri Jan 29 06:58:35 2021\n",
            "  step 10700 lr 1 step-time 0.13s wps 43.34K ppl 15.71 gN 5.30 bleu 14.77, Fri Jan 29 06:58:48 2021\n",
            "  step 10800 lr 1 step-time 0.12s wps 44.62K ppl 15.54 gN 5.31 bleu 14.77, Fri Jan 29 06:59:01 2021\n",
            "  step 10900 lr 1 step-time 0.13s wps 42.03K ppl 15.61 gN 5.33 bleu 14.77, Fri Jan 29 06:59:14 2021\n",
            "  step 11000 lr 1 step-time 0.13s wps 43.47K ppl 15.59 gN 5.30 bleu 14.77, Fri Jan 29 06:59:27 2021\n",
            "# Save eval, global step 11000\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-11000\n",
            "I0129 06:59:27.840037 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-11000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-11000, time 0.06s\n",
            "  # 149\n",
            "    src: bà tôi thì đang ngồi đối diện nhìn tôi chằm chằm .\n",
            "    ref: And my grandmother was sitting across the room staring at me .\n",
            "    nmt: She was sitting on my face .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-11000\n",
            "I0129 06:59:27.935641 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-11000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-11000, time 0.07s\n",
            "  eval dev: perplexity 16.45, time 1s, Fri Jan 29 06:59:29 2021.\n",
            "  eval test: perplexity 15.01, time 1s, Fri Jan 29 06:59:30 2021.\n",
            "  step 11100 lr 1 step-time 0.13s wps 42.98K ppl 15.90 gN 5.49 bleu 14.77, Fri Jan 29 06:59:43 2021\n",
            "  step 11200 lr 1 step-time 0.13s wps 43.18K ppl 15.93 gN 5.33 bleu 14.77, Fri Jan 29 06:59:56 2021\n",
            "  step 11300 lr 1 step-time 0.13s wps 43.24K ppl 15.90 gN 5.46 bleu 14.77, Fri Jan 29 07:00:09 2021\n",
            "  step 11400 lr 1 step-time 0.12s wps 45.19K ppl 15.70 gN 5.36 bleu 14.77, Fri Jan 29 07:00:22 2021\n",
            "# Finished an epoch, step 11473. Perform external evaluation\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-11000\n",
            "I0129 07:00:31.883972 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-11000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-11000, time 0.06s\n",
            "  # 733\n",
            "    src: Và ở đó , một y tá khác , không phải y tá chăm sóc bà Drucker lúc trước , mà là một y tá khác , nói với tôi 3 chữ đó là 3 chữ mà hầu hết bác sĩ cấp cứu tôi biết đều kinh hãi .\n",
            "    ref: And it was there that another nurse , not the nurse who was looking after Mrs. Drucker before , but another nurse , said three words to me that are the three words that most emergency physicians I know dread .\n",
            "    nmt: And there is another nurse , another nurse , not a nurse , and it &apos;s a nurse , which is a nurse , said to me three words that are three words that most of my doctors know are absolutely awesome .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-11000\n",
            "I0129 07:00:32.059388 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-11000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-11000, time 0.07s\n",
            "# External evaluation, global step 11000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 11s, Fri Jan 29 07:00:43 2021.\n",
            "  bleu dev: 13.5\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 11000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 10s, Fri Jan 29 07:00:54 2021.\n",
            "  bleu test: 15.0\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  step 11500 lr 1 step-time 0.17s wps 31.78K ppl 15.23 gN 5.65 bleu 14.77, Fri Jan 29 07:01:02 2021\n",
            "  step 11600 lr 1 step-time 0.26s wps 21.34K ppl 14.65 gN 5.32 bleu 14.77, Fri Jan 29 07:01:28 2021\n",
            "  step 11700 lr 1 step-time 0.17s wps 32.09K ppl 14.83 gN 5.30 bleu 14.77, Fri Jan 29 07:01:46 2021\n",
            "  step 11800 lr 1 step-time 0.13s wps 44.77K ppl 14.76 gN 5.22 bleu 14.77, Fri Jan 29 07:01:58 2021\n",
            "  step 11900 lr 1 step-time 0.13s wps 43.78K ppl 14.97 gN 5.24 bleu 14.77, Fri Jan 29 07:02:11 2021\n",
            "  step 12000 lr 1 step-time 0.13s wps 42.62K ppl 14.85 gN 5.34 bleu 14.77, Fri Jan 29 07:02:24 2021\n",
            "# Save eval, global step 12000\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-12000\n",
            "I0129 07:02:24.979259 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-12000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-12000, time 0.06s\n",
            "  # 187\n",
            "    src: Tôi biết rằng buổi nói chuyện này sẽ được phát sóng rộng rãi\n",
            "    ref: I know this might be broadcast broadly .\n",
            "    nmt: I know that this talk will be a huge <unk> .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-12000\n",
            "I0129 07:02:25.083307 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-12000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-12000, time 0.07s\n",
            "  eval dev: perplexity 16.39, time 1s, Fri Jan 29 07:02:26 2021.\n",
            "  eval test: perplexity 16.12, time 1s, Fri Jan 29 07:02:27 2021.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-12000\n",
            "I0129 07:02:27.727302 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-12000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-12000, time 0.06s\n",
            "  # 1220\n",
            "    src: Nhưng những con số chẳng là gì cả .\n",
            "    ref: And that &apos;s nothing .\n",
            "    nmt: But the numbers of the numbers are not anything .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-12000\n",
            "I0129 07:02:27.833813 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-12000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-12000, time 0.07s\n",
            "  eval dev: perplexity 16.39, time 1s, Fri Jan 29 07:02:29 2021.\n",
            "  eval test: perplexity 16.12, time 1s, Fri Jan 29 07:02:30 2021.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-12000\n",
            "I0129 07:02:30.396013 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-12000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-12000, time 0.06s\n",
            "# External evaluation, global step 12000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 17s, Fri Jan 29 07:02:47 2021.\n",
            "  bleu dev: 8.9\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 12000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 16s, Fri Jan 29 07:03:04 2021.\n",
            "  bleu test: 7.6\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# Final, step 12000 lr 1 step-time 0.13s wps 42.62K ppl 14.85 gN 5.34 dev ppl 16.39, dev bleu 8.9, test ppl 16.12, test bleu 7.6, Fri Jan 29 07:03:04 2021\n",
            "# Done training!, time 2195s, Fri Jan 29 07:03:04 2021.\n",
            "# Start evaluating saved best models.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/best_bleu/translate.ckpt-10000\n",
            "I0129 07:03:04.904140 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/best_bleu/translate.ckpt-10000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/best_bleu/translate.ckpt-10000, time 0.07s\n",
            "  # 903\n",
            "    src: Có cả những bản phối khí lại .\n",
            "    ref: There were remixes .\n",
            "    nmt: There are the <unk> of the air .\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/best_bleu/translate.ckpt-10000\n",
            "I0129 07:03:05.008033 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/best_bleu/translate.ckpt-10000\n",
            "  loaded eval model parameters from /tmp/nmt_attention_model/best_bleu/translate.ckpt-10000, time 0.07s\n",
            "  eval dev: perplexity 15.46, time 1s, Fri Jan 29 07:03:06 2021.\n",
            "  eval test: perplexity 14.38, time 1s, Fri Jan 29 07:03:07 2021.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/best_bleu/translate.ckpt-10000\n",
            "I0129 07:03:07.590256 139824990775168 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/best_bleu/translate.ckpt-10000\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/best_bleu/translate.ckpt-10000, time 0.06s\n",
            "# External evaluation, global step 10000\n",
            "  decoding to output /tmp/nmt_attention_model/output_dev\n",
            "  done, num sentences 1553, num translations per input 1, time 8s, Fri Jan 29 07:03:16 2021.\n",
            "  bleu dev: 14.8\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# External evaluation, global step 10000\n",
            "  decoding to output /tmp/nmt_attention_model/output_test\n",
            "  done, num sentences 1268, num translations per input 1, time 8s, Fri Jan 29 07:03:25 2021.\n",
            "  bleu test: 16.7\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "# Best bleu, step 10000 lr 1 step-time 0.13s wps 42.62K ppl 14.85 gN 5.34 dev ppl 15.46, dev bleu 14.8, test ppl 14.38, test bleu 16.7, Fri Jan 29 07:03:25 2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV5aRmht0sgN",
        "outputId": "c6abeea4-74f8-40e9-941d-e5ed2c54f6da"
      },
      "source": [
        "# 查看一下可以调整的参数\r\n",
        "!python3 -m nmt.nmt.nmt --help"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: nmt.py [-h] [--num_units NUM_UNITS] [--num_layers NUM_LAYERS]\n",
            "              [--num_encoder_layers NUM_ENCODER_LAYERS]\n",
            "              [--num_decoder_layers NUM_DECODER_LAYERS]\n",
            "              [--encoder_type ENCODER_TYPE] [--residual [RESIDUAL]]\n",
            "              [--time_major [TIME_MAJOR]]\n",
            "              [--num_embeddings_partitions NUM_EMBEDDINGS_PARTITIONS]\n",
            "              [--attention ATTENTION]\n",
            "              [--attention_architecture ATTENTION_ARCHITECTURE]\n",
            "              [--output_attention [OUTPUT_ATTENTION]]\n",
            "              [--pass_hidden_state [PASS_HIDDEN_STATE]]\n",
            "              [--optimizer OPTIMIZER] [--learning_rate LEARNING_RATE]\n",
            "              [--warmup_steps WARMUP_STEPS] [--warmup_scheme WARMUP_SCHEME]\n",
            "              [--decay_scheme DECAY_SCHEME]\n",
            "              [--num_train_steps NUM_TRAIN_STEPS]\n",
            "              [--colocate_gradients_with_ops [COLOCATE_GRADIENTS_WITH_OPS]]\n",
            "              [--init_op INIT_OP] [--init_weight INIT_WEIGHT] [--src SRC]\n",
            "              [--tgt TGT] [--train_prefix TRAIN_PREFIX]\n",
            "              [--dev_prefix DEV_PREFIX] [--test_prefix TEST_PREFIX]\n",
            "              [--out_dir OUT_DIR] [--vocab_prefix VOCAB_PREFIX]\n",
            "              [--embed_prefix EMBED_PREFIX] [--sos SOS] [--eos EOS]\n",
            "              [--share_vocab [SHARE_VOCAB]]\n",
            "              [--check_special_token CHECK_SPECIAL_TOKEN]\n",
            "              [--src_max_len SRC_MAX_LEN] [--tgt_max_len TGT_MAX_LEN]\n",
            "              [--src_max_len_infer SRC_MAX_LEN_INFER]\n",
            "              [--tgt_max_len_infer TGT_MAX_LEN_INFER] [--unit_type UNIT_TYPE]\n",
            "              [--forget_bias FORGET_BIAS] [--dropout DROPOUT]\n",
            "              [--max_gradient_norm MAX_GRADIENT_NORM]\n",
            "              [--batch_size BATCH_SIZE] [--steps_per_stats STEPS_PER_STATS]\n",
            "              [--max_train MAX_TRAIN] [--num_buckets NUM_BUCKETS]\n",
            "              [--num_sampled_softmax NUM_SAMPLED_SOFTMAX]\n",
            "              [--subword_option {,bpe,spm}]\n",
            "              [--use_char_encode USE_CHAR_ENCODE] [--num_gpus NUM_GPUS]\n",
            "              [--log_device_placement [LOG_DEVICE_PLACEMENT]]\n",
            "              [--metrics METRICS]\n",
            "              [--steps_per_external_eval STEPS_PER_EXTERNAL_EVAL]\n",
            "              [--scope SCOPE] [--hparams_path HPARAMS_PATH]\n",
            "              [--random_seed RANDOM_SEED]\n",
            "              [--override_loaded_hparams [OVERRIDE_LOADED_HPARAMS]]\n",
            "              [--num_keep_ckpts NUM_KEEP_CKPTS] [--avg_ckpts [AVG_CKPTS]]\n",
            "              [--language_model [LANGUAGE_MODEL]] [--ckpt CKPT]\n",
            "              [--inference_input_file INFERENCE_INPUT_FILE]\n",
            "              [--inference_list INFERENCE_LIST]\n",
            "              [--infer_batch_size INFER_BATCH_SIZE]\n",
            "              [--inference_output_file INFERENCE_OUTPUT_FILE]\n",
            "              [--inference_ref_file INFERENCE_REF_FILE]\n",
            "              [--infer_mode {greedy,sample,beam_search}]\n",
            "              [--beam_width BEAM_WIDTH]\n",
            "              [--length_penalty_weight LENGTH_PENALTY_WEIGHT]\n",
            "              [--coverage_penalty_weight COVERAGE_PENALTY_WEIGHT]\n",
            "              [--sampling_temperature SAMPLING_TEMPERATURE]\n",
            "              [--num_translations_per_input NUM_TRANSLATIONS_PER_INPUT]\n",
            "              [--jobid JOBID] [--num_workers NUM_WORKERS]\n",
            "              [--num_inter_threads NUM_INTER_THREADS]\n",
            "              [--num_intra_threads NUM_INTRA_THREADS]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --num_units NUM_UNITS\n",
            "                        Network size.\n",
            "  --num_layers NUM_LAYERS\n",
            "                        Network depth.\n",
            "  --num_encoder_layers NUM_ENCODER_LAYERS\n",
            "                        Encoder depth, equal to num_layers if None.\n",
            "  --num_decoder_layers NUM_DECODER_LAYERS\n",
            "                        Decoder depth, equal to num_layers if None.\n",
            "  --encoder_type ENCODER_TYPE\n",
            "                        uni | bi | gnmt. For bi, we build num_encoder_layers/2\n",
            "                        bi-directional layers. For gnmt, we build 1 bi-\n",
            "                        directional layer, and (num_encoder_layers - 1) uni-\n",
            "                        directional layers.\n",
            "  --residual [RESIDUAL]\n",
            "                        Whether to add residual connections.\n",
            "  --time_major [TIME_MAJOR]\n",
            "                        Whether to use time-major mode for dynamic RNN.\n",
            "  --num_embeddings_partitions NUM_EMBEDDINGS_PARTITIONS\n",
            "                        Number of partitions for embedding vars.\n",
            "  --attention ATTENTION\n",
            "                        luong | scaled_luong | bahdanau | normed_bahdanau or\n",
            "                        set to \"\" for no attention\n",
            "  --attention_architecture ATTENTION_ARCHITECTURE\n",
            "                        standard | gnmt | gnmt_v2. standard: use top layer to\n",
            "                        compute attention. gnmt: GNMT style of computing\n",
            "                        attention, use previous bottom layer to compute\n",
            "                        attention. gnmt_v2: similar to gnmt, but use current\n",
            "                        bottom layer to compute attention.\n",
            "  --output_attention [OUTPUT_ATTENTION]\n",
            "                        Only used in standard attention_architecture. Whether\n",
            "                        use attention as the cell output at each timestep. .\n",
            "  --pass_hidden_state [PASS_HIDDEN_STATE]\n",
            "                        Whether to pass encoder's hidden state to decoder when\n",
            "                        using an attention based model.\n",
            "  --optimizer OPTIMIZER\n",
            "                        sgd | adam\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        Learning rate. Adam: 0.001 | 0.0001\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        How many steps we inverse-decay learning.\n",
            "  --warmup_scheme WARMUP_SCHEME\n",
            "                        How to warmup learning rates. Options include: t2t:\n",
            "                        Tensor2Tensor's way, start with lr 100 times smaller,\n",
            "                        then exponentiate until the specified lr.\n",
            "  --decay_scheme DECAY_SCHEME\n",
            "                        How we decay learning rate. Options include: luong234:\n",
            "                        after 2/3 num train steps, we start halving the\n",
            "                        learning rate for 4 times before finishing. luong5:\n",
            "                        after 1/2 num train steps, we start halving the\n",
            "                        learning rate for 5 times before finishing. luong10:\n",
            "                        after 1/2 num train steps, we start halving the\n",
            "                        learning rate for 10 times before finishing.\n",
            "  --num_train_steps NUM_TRAIN_STEPS\n",
            "                        Num steps to train.\n",
            "  --colocate_gradients_with_ops [COLOCATE_GRADIENTS_WITH_OPS]\n",
            "                        Whether try colocating gradients with corresponding op\n",
            "  --init_op INIT_OP     uniform | glorot_normal | glorot_uniform\n",
            "  --init_weight INIT_WEIGHT\n",
            "                        for uniform init_op, initialize weights between\n",
            "                        [-this, this].\n",
            "  --src SRC             Source suffix, e.g., en.\n",
            "  --tgt TGT             Target suffix, e.g., de.\n",
            "  --train_prefix TRAIN_PREFIX\n",
            "                        Train prefix, expect files with src/tgt suffixes.\n",
            "  --dev_prefix DEV_PREFIX\n",
            "                        Dev prefix, expect files with src/tgt suffixes.\n",
            "  --test_prefix TEST_PREFIX\n",
            "                        Test prefix, expect files with src/tgt suffixes.\n",
            "  --out_dir OUT_DIR     Store log/model files.\n",
            "  --vocab_prefix VOCAB_PREFIX\n",
            "                        Vocab prefix, expect files with src/tgt suffixes.\n",
            "  --embed_prefix EMBED_PREFIX\n",
            "                        Pretrained embedding prefix, expect files with src/tgt\n",
            "                        suffixes. The embedding files should be Glove formated\n",
            "                        txt files.\n",
            "  --sos SOS             Start-of-sentence symbol.\n",
            "  --eos EOS             End-of-sentence symbol.\n",
            "  --share_vocab [SHARE_VOCAB]\n",
            "                        Whether to use the source vocab and embeddings for\n",
            "                        both source and target.\n",
            "  --check_special_token CHECK_SPECIAL_TOKEN\n",
            "                        Whether check special sos, eos, unk tokens exist in\n",
            "                        the vocab files.\n",
            "  --src_max_len SRC_MAX_LEN\n",
            "                        Max length of src sequences during training.\n",
            "  --tgt_max_len TGT_MAX_LEN\n",
            "                        Max length of tgt sequences during training.\n",
            "  --src_max_len_infer SRC_MAX_LEN_INFER\n",
            "                        Max length of src sequences during inference.\n",
            "  --tgt_max_len_infer TGT_MAX_LEN_INFER\n",
            "                        Max length of tgt sequences during inference. Also use\n",
            "                        to restrict the maximum decoding length.\n",
            "  --unit_type UNIT_TYPE\n",
            "                        lstm | gru | layer_norm_lstm | nas\n",
            "  --forget_bias FORGET_BIAS\n",
            "                        Forget bias for BasicLSTMCell.\n",
            "  --dropout DROPOUT     Dropout rate (not keep_prob)\n",
            "  --max_gradient_norm MAX_GRADIENT_NORM\n",
            "                        Clip gradients to this norm.\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size.\n",
            "  --steps_per_stats STEPS_PER_STATS\n",
            "                        How many training steps to do per stats logging.Save\n",
            "                        checkpoint every 10x steps_per_stats\n",
            "  --max_train MAX_TRAIN\n",
            "                        Limit on the size of training data (0: no limit).\n",
            "  --num_buckets NUM_BUCKETS\n",
            "                        Put data into similar-length buckets.\n",
            "  --num_sampled_softmax NUM_SAMPLED_SOFTMAX\n",
            "                        Use sampled_softmax_loss if > 0.Otherwise, use full\n",
            "                        softmax loss.\n",
            "  --subword_option {,bpe,spm}\n",
            "                        Set to bpe or spm to activate subword desegmentation.\n",
            "  --use_char_encode USE_CHAR_ENCODE\n",
            "                        Whether to split each word or bpe into character, and\n",
            "                        then generate the word-level representation from the\n",
            "                        character reprentation.\n",
            "  --num_gpus NUM_GPUS   Number of gpus in each worker.\n",
            "  --log_device_placement [LOG_DEVICE_PLACEMENT]\n",
            "                        Debug GPU allocation.\n",
            "  --metrics METRICS     Comma-separated list of evaluations metrics\n",
            "                        (bleu,rouge,accuracy)\n",
            "  --steps_per_external_eval STEPS_PER_EXTERNAL_EVAL\n",
            "                        How many training steps to do per external evaluation.\n",
            "                        Automatically set based on data if None.\n",
            "  --scope SCOPE         scope to put variables under\n",
            "  --hparams_path HPARAMS_PATH\n",
            "                        Path to standard hparams json file that\n",
            "                        overrideshparams values from FLAGS.\n",
            "  --random_seed RANDOM_SEED\n",
            "                        Random seed (>0, set a specific seed).\n",
            "  --override_loaded_hparams [OVERRIDE_LOADED_HPARAMS]\n",
            "                        Override loaded hparams with values specified\n",
            "  --num_keep_ckpts NUM_KEEP_CKPTS\n",
            "                        Max number of checkpoints to keep.\n",
            "  --avg_ckpts [AVG_CKPTS]\n",
            "                        Average the last N checkpoints for external\n",
            "                        evaluation. N can be controlled by setting\n",
            "                        --num_keep_ckpts.\n",
            "  --language_model [LANGUAGE_MODEL]\n",
            "                        True to train a language model, ignoring encoder\n",
            "  --ckpt CKPT           Checkpoint file to load a model for inference.\n",
            "  --inference_input_file INFERENCE_INPUT_FILE\n",
            "                        Set to the text to decode.\n",
            "  --inference_list INFERENCE_LIST\n",
            "                        A comma-separated list of sentence indices (0-based)\n",
            "                        to decode.\n",
            "  --infer_batch_size INFER_BATCH_SIZE\n",
            "                        Batch size for inference mode.\n",
            "  --inference_output_file INFERENCE_OUTPUT_FILE\n",
            "                        Output file to store decoding results.\n",
            "  --inference_ref_file INFERENCE_REF_FILE\n",
            "                        Reference file to compute evaluation scores (if\n",
            "                        provided).\n",
            "  --infer_mode {greedy,sample,beam_search}\n",
            "                        Which type of decoder to use during inference.\n",
            "  --beam_width BEAM_WIDTH\n",
            "                        beam width when using beam search decoder. If 0\n",
            "                        (default), use standard decoder with greedy helper.\n",
            "  --length_penalty_weight LENGTH_PENALTY_WEIGHT\n",
            "                        Length penalty for beam search.\n",
            "  --coverage_penalty_weight COVERAGE_PENALTY_WEIGHT\n",
            "                        Coverage penalty for beam search.\n",
            "  --sampling_temperature SAMPLING_TEMPERATURE\n",
            "                        Softmax sampling temperature for inference decoding,\n",
            "                        0.0 means greedy decoding. This option is ignored when\n",
            "                        using beam search.\n",
            "  --num_translations_per_input NUM_TRANSLATIONS_PER_INPUT\n",
            "                        Number of translations generated for each sentence.\n",
            "                        This is only used for inference.\n",
            "  --jobid JOBID         Task id of the worker.\n",
            "  --num_workers NUM_WORKERS\n",
            "                        Number of workers (inference only).\n",
            "  --num_inter_threads NUM_INTER_THREADS\n",
            "                        number of inter_op_parallelism_threads\n",
            "  --num_intra_threads NUM_INTRA_THREADS\n",
            "                        number of intra_op_parallelism_threads\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfRbf9gX3cGd"
      },
      "source": [
        "# 训练好的模型进行测试，可以这样做\r\n",
        "# 首先在data下新建一个文件，比如my_infer_file.vi\r\n",
        "# 然后从tst2013.vi复制一些句子到其中，用作要被翻译的句子。"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci-y76QyFDPD",
        "outputId": "ff3e01a9-5711-4638-8174-3540c3f00685"
      },
      "source": [
        "!python3 -m nmt.nmt.nmt \\\r\n",
        "    --out_dir=/tmp/nmt_attention_model \\\r\n",
        "    --vocab_prefix=./data/vocab  \\\r\n",
        "    --inference_input_file=./data/my_infer_file.vi \\\r\n",
        "    --inference_output_file=./output_infer"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/nmt/nmt/nmt.py:707: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0129 07:41:36.001784 139642247858048 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "I0129 07:41:36.438718 139642247858048 utils.py:157] NumExpr defaulting to 2 threads.\n",
            "# Job id 0\n",
            "WARNING:tensorflow:From /content/nmt/nmt/nmt.py:629: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0129 07:41:36.705005 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/nmt.py:629: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-01-29 07:41:36.706427: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-01-29 07:41:36.738944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:36.739489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-01-29 07:41:36.739762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-01-29 07:41:36.741209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-01-29 07:41:36.742839: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-01-29 07:41:36.743196: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-01-29 07:41:36.744903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-01-29 07:41:36.745759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-01-29 07:41:36.749215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-01-29 07:41:36.749343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:36.749901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:36.750413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-01-29 07:41:36.755377: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-01-29 07:41:36.755557: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x22bef40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-01-29 07:41:36.755585: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-01-29 07:41:36.855134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:36.855788: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x22bf100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-01-29 07:41:36.855820: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-01-29 07:41:36.855987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:36.856513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-01-29 07:41:36.856581: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-01-29 07:41:36.856612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-01-29 07:41:36.856633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-01-29 07:41:36.856655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-01-29 07:41:36.856682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-01-29 07:41:36.856702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-01-29 07:41:36.856721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-01-29 07:41:36.856797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:36.857377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:36.857859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-01-29 07:41:36.857922: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-01-29 07:41:36.859031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-01-29 07:41:36.859060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-01-29 07:41:36.859073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-01-29 07:41:36.859186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:36.859765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:36.860254: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-01-29 07:41:36.860295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 10475843884879960330), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7196038326195552626), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 12101994116370383661), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 14912199066, 10065036389403962027)]\n",
            "WARNING:tensorflow:From /content/nmt/nmt/nmt.py:640: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n",
            "W0129 07:41:36.861412 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/nmt.py:640: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n",
            "# Loading hparams from /tmp/nmt_attention_model/hparams\n",
            "WARNING:tensorflow:From /content/nmt/nmt/utils/misc_utils.py:94: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0129 07:41:36.861761 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/utils/misc_utils.py:94: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "# Vocab file ./data/vocab.vi exists\n",
            "# Vocab file ./data/vocab.en exists\n",
            "WARNING:tensorflow:From /content/nmt/nmt/nmt.py:548: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0129 07:41:36.892499 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/nmt.py:548: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "  saving hparams to /tmp/nmt_attention_model/hparams\n",
            "  saving hparams to /tmp/nmt_attention_model/best_bleu/hparams\n",
            "  attention=scaled_luong\n",
            "  attention_architecture=standard\n",
            "  avg_ckpts=False\n",
            "  batch_size=128\n",
            "  beam_width=0\n",
            "  best_bleu=14.772658168036237\n",
            "  best_bleu_dir=/tmp/nmt_attention_model/best_bleu\n",
            "  check_special_token=True\n",
            "  colocate_gradients_with_ops=True\n",
            "  coverage_penalty_weight=0.0\n",
            "  decay_scheme=\n",
            "  dev_prefix=./data/tst2012\n",
            "  dropout=0.2\n",
            "  embed_prefix=None\n",
            "  encoder_type=uni\n",
            "  eos=</s>\n",
            "  epoch_step=527\n",
            "  forget_bias=1.0\n",
            "  infer_batch_size=32\n",
            "  infer_mode=greedy\n",
            "  init_op=uniform\n",
            "  init_weight=0.1\n",
            "  language_model=False\n",
            "  learning_rate=1.0\n",
            "  length_penalty_weight=0.0\n",
            "  log_device_placement=False\n",
            "  max_gradient_norm=5.0\n",
            "  max_train=0\n",
            "  metrics=['bleu']\n",
            "  num_buckets=5\n",
            "  num_dec_emb_partitions=0\n",
            "  num_decoder_layers=2\n",
            "  num_decoder_residual_layers=0\n",
            "  num_embeddings_partitions=0\n",
            "  num_enc_emb_partitions=0\n",
            "  num_encoder_layers=2\n",
            "  num_encoder_residual_layers=0\n",
            "  num_gpus=1\n",
            "  num_inter_threads=0\n",
            "  num_intra_threads=0\n",
            "  num_keep_ckpts=5\n",
            "  num_sampled_softmax=0\n",
            "  num_train_steps=12000\n",
            "  num_translations_per_input=1\n",
            "  num_units=128\n",
            "  optimizer=sgd\n",
            "  out_dir=/tmp/nmt_attention_model\n",
            "  output_attention=True\n",
            "  override_loaded_hparams=False\n",
            "  pass_hidden_state=True\n",
            "  random_seed=None\n",
            "  residual=False\n",
            "  sampling_temperature=0.0\n",
            "  share_vocab=False\n",
            "  sos=<s>\n",
            "  src=vi\n",
            "  src_embed_file=\n",
            "  src_max_len=50\n",
            "  src_max_len_infer=None\n",
            "  src_vocab_file=./data/vocab.vi\n",
            "  src_vocab_size=7709\n",
            "  steps_per_external_eval=None\n",
            "  steps_per_stats=100\n",
            "  subword_option=\n",
            "  test_prefix=./data/tst2013\n",
            "  tgt=en\n",
            "  tgt_embed_file=\n",
            "  tgt_max_len=50\n",
            "  tgt_max_len_infer=None\n",
            "  tgt_vocab_file=./data/vocab.en\n",
            "  tgt_vocab_size=17191\n",
            "  time_major=True\n",
            "  train_prefix=./data/train\n",
            "  unit_type=lstm\n",
            "  use_char_encode=False\n",
            "  vocab_prefix=./data/vocab\n",
            "  warmup_scheme=t2t\n",
            "  warmup_steps=0\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:202: The name tf.container is deprecated. Please use tf.compat.v1.container instead.\n",
            "\n",
            "W0129 07:41:36.895572 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:202: The name tf.container is deprecated. Please use tf.compat.v1.container instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:208: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0129 07:41:36.903337 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:208: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:Entity <function get_infer_iterator.<locals>.<lambda> at 0x7f00b043cae8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 07:41:36.923803 139642247858048 ag_logging.py:146] Entity <function get_infer_iterator.<locals>.<lambda> at 0x7f00b043cae8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_infer_iterator.<locals>.<lambda> at 0x7f008ec28488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 07:41:36.938754 139642247858048 ag_logging.py:146] Entity <function get_infer_iterator.<locals>.<lambda> at 0x7f008ec28488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <function get_infer_iterator.<locals>.<lambda> at 0x7f008ebefe18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0129 07:41:36.947713 139642247858048 ag_logging.py:146] Entity <function get_infer_iterator.<locals>.<lambda> at 0x7f008ebefe18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:From /content/nmt/nmt/utils/iterator_utils.py:87: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W0129 07:41:36.956516 139642247858048 deprecation.py:323] From /content/nmt/nmt/utils/iterator_utils.py:87: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:162: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "W0129 07:41:36.963713 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/model.py:162: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0129 07:41:36.963928 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:285: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0129 07:41:36.964301 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:285: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "# Creating infer graph ...\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:375: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n",
            "W0129 07:41:36.977119 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/model.py:375: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n",
            "# Build a basic encoder\n",
            "  num_layers = 2, num_residual_layers=0\n",
            "  cell 0  LSTM, forget_bias=1WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:402: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0129 07:41:36.981022 139642247858048 deprecation.py:323] From /content/nmt/nmt/model_helper.py:402: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "  DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:508: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "W0129 07:41:36.982084 139642247858048 deprecation.py:323] From /content/nmt/nmt/model_helper.py:508: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:767: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0129 07:41:36.982499 139642247858048 deprecation.py:323] From /content/nmt/nmt/model.py:767: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "W0129 07:41:37.051853 139642247858048 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0129 07:41:37.059464 139642247858048 deprecation.py:506] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0129 07:41:37.092357 139642247858048 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:445: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0129 07:41:37.113638 139642247858048 deprecation.py:323] From /content/nmt/nmt/model.py:445: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:445: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0129 07:41:37.115519 139642247858048 deprecation.py:323] From /content/nmt/nmt/model.py:445: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:183: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0129 07:41:37.528457 139642247858048 deprecation.py:323] From /content/nmt/nmt/model.py:183: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:190: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "W0129 07:41:37.529762 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/model.py:190: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/attention_model.py:193: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "W0129 07:41:37.536234 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/attention_model.py:193: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "# Trainable variables\n",
            "Format: <name>, <shape>, <(soft) device placement>\n",
            "  embeddings/encoder/embedding_encoder:0, (7709, 128), /device:GPU:0\n",
            "  embeddings/decoder/embedding_decoder:0, (17191, 128), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 17191), \n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:100: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0129 07:41:37.538525 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/model.py:100: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model.py:101: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "W0129 07:41:37.538725 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/model.py:101: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/nmt/nmt/utils/misc_utils.py:142: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0129 07:41:37.555980 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/utils/misc_utils.py:142: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "2021-01-29 07:41:37.556494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:37.557047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-01-29 07:41:37.557122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-01-29 07:41:37.557151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-01-29 07:41:37.557173: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-01-29 07:41:37.557193: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-01-29 07:41:37.557217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-01-29 07:41:37.557235: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-01-29 07:41:37.557254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-01-29 07:41:37.557347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:37.557868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:37.558377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-01-29 07:41:37.558419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-01-29 07:41:37.558434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-01-29 07:41:37.558447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-01-29 07:41:37.558543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:37.559051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-01-29 07:41:37.559685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-12000\n",
            "I0129 07:41:37.560838 139642247858048 saver.py:1284] Restoring parameters from /tmp/nmt_attention_model/translate.ckpt-12000\n",
            "WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:541: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
            "\n",
            "W0129 07:41:37.641072 139642247858048 module_wrapper.py:139] From /content/nmt/nmt/model_helper.py:541: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
            "\n",
            "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-12000, time 0.12s\n",
            "# Start decoding\n",
            "  decoding to output ./output_infer\n",
            "2021-01-29 07:41:38.360568: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "  done, num sentences 10, num translations per input 1, time 0s, Fri Jan 29 07:41:38 2021.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGhs4E9wF7r8",
        "outputId": "a6b7ab6d-491f-4142-a788-087388b8256b"
      },
      "source": [
        "# 查看一下生成的文件\r\n",
        "!cat ./output_infer # To view the inference as output"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When I was a little kid , I think the <unk> of the world is the best of the world and I &apos;m going to sing the &quot; We have no <unk> . &quot;\n",
            "I was so proud of the country .\n",
            "In the school , we have a lot of time to learn the <unk> of the <unk> of the <unk> <unk> , but the <unk> of the world , the <unk> of the world , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the\n",
            "Even though I &apos;ve been wondering that the world is not going to know how to the outside of the world , but I think I &apos;d be able to live the whole life of the <unk> , until the <unk> of the <unk> .\n",
            "When I was seven , I saw the <unk> of the people who <unk> the first time , but I think the life of my life is completely <unk> .\n",
            "My family is not poor , and I have not been a lot of <unk> .\n",
            "But in the course of the year of 1995 , my mother was brought home to a woman from the same time with the mother .\n",
            "In the case of the <unk> , when you read the <unk> of the <unk> , the <unk> of the children of the children , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk> , the <unk> of the <unk>\n",
            "All of the same is the same , and the body we can feel like the death of the dying is coming to be very close to the very <unk> .\n",
            "I was shocked .\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}